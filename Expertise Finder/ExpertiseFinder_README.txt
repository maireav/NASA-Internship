Hi ! Thanks for accessing my files. This document contains a bunch of information and explanation of how my code works. Please take the time to read over the NECESSARY PACKAGES & ITEMS and the ABOUT THE CODE sections. If you come across specific questions when running, please take a look at THE DETAILS section in case your questions has already been addressed there. Thanks !

——————————————————————————————————————————————————

NECESSARY PACKAGES & ITEMS
These modules and packages should be installed before attempting to run my code:
- pandas
- ads
- operator
- re
- nltk
    - nltk.tokenize
    - nltk.stem
- fnmatch

These are the primary .py files that make up the “Expertise Finder”:
- ExpertiseFinder_MSI.py
- TextAnalysis.py

The user may use their own file containing stop words for language processing while using the “Expertise Finder”, or use the file provided on GitHub:
- stopwords.txt

PLEASE NOTE: A user must also have their own NASA ADS API token. This is a long string of characters generated by ADS that gives you access to their API. You can find instructions on how to get an ADS token here: https://ads.readthedocs.io/en/v1/api-key.html.

——————————————————————————————————————————————————

ABOUT THE CODE
This collection of functions and files was developed over several months while working as a USRA intern at NASA HQ. It was built in response to NASA’s call to expand and improve the diversity of its pool of potential proposal reviewers. The “Expertise Finder” and its component/associated functions query Harvard-NASA’s ADS API to determine the expertise of a researcher based on their name and/or institution and the most common words and phrases found in the abstracts of papers published on NASA ADS.

This code is heavily based on functions within the ads python package written by Vladimir Sudilovsky & Andy Casey, Geert Barentsen, Dan Foreman-Mackey, Miguel de Val-Borro, and Jonny Elliott. Please see more documentation on their work here: https://ads.readthedocs.io/en/latest/. I cannot thank these coders enough for making my code a possibility. 

——————————————————————————————————————————————————

THE DETAILS
In this section, you will find a (relatively) detailed description of the purpose and functionality of each function contained within the file “ExpertiseFinder_MSI.py” and "ExpertiseFinder_Twitter.py", beginning with the primary functions of the package.

—THE “EXPERTISE FINDER”—
All functions beginning with “expertiseFinder” all serve a similar purpose: take in a file or string containing names and/or affiliate institutions, search the name and/or institution information on NASA ADS, and then return an output containing the input data and, in addition, the top words and phrases from each researcher’s papers published on ADS. 
But how does the expertise finder pick papers from ADS ? The “strictness” keyword argument determines the sequence of filters that will be used to make this choice. NASA ADS is slightly “messy” when returning results in a name search. Sometimes, ADS has results that have incorrect names and/or institutions, other times, the name or institution may be correct, but come from non-astronomical journals. The “filters” that occur in step 6 of all of the “expertise finder” functions are designed to remove erroneous search results. In some (not all) of the expertise finder functions, the user will be able to choose between “low”, “high”, and “default” strictness. Note that this customization is not available for the functions within the Twitter .py file. Please read the description of how each level affects the filters within the individual functions in the .py file itself. Here is a quick summary:
- LOW strictness triggers filters that permit an exact author match, a close institution match, or the presence of a journal name (ApJ, MNRAS, SPIE, AJ, Science, PASP, Nature, Arxiv). 
- HIGH strictness triggers filters that permit a relevant journal name AND an exact author match or a close institution match (the provided institution name string is included in the official affiliation name). 
- DEFAULT strictness triggers filters that permit a close author match AND close institution match, OR an exact author match, OR a close institution match, OR a relevant journal name.
The user will also notice that, for the majority of expertise finders, they return two items: one “dirty”, one “clean”. Some expertise finders instead return a single data frame with a "dirty" or "clean" data type column. The “clean” data refers to the data pulled from papers that DID meet the filter criteria. These papers and their data are considered to be affiliated with the author and institution defined in the input. The “dirty” data contains the papers and top words that DID NOT meet the filter criteria.

Below is a discussion of how each expertise finder in "ExpertiseFinder_MSI.py" and "ExpertiseFinder_Twitter.py" is different, and in which context they are most useful.
IN THE MSI FILE--
expertiseFinder(): Best for processing a large spreadsheet of names and institutions. Offers the most customizable strictness options.
expertiseFinder_lowStrictness(): Best for processing a large spreadsheet of names and institutions. Has the least stingiest filters when sorting papers, so it may be useful to use for less common names and/or institutions. Offers no further customization in terms of filters.
expertiseFinder_highStrictness(): Best for processing a large spreadsheet of names and institutions. Has the most stingiest filters when sorting papers; it queries both name AND affiliation when searching ads. It may be useful when searching for more common names and/or institutions. Offers no further customization of filters.
expertiseFinder_singleName(): This function is made for processing a single string for a researcher’s name, and a single string of their affiliated institution. It is best for finding information on only one person. This function also offers customization in strictness to accommodate more common and rare names & institutions.
instFinder(): Although this function does not have “expertise finder” in its name, it is still related. This function takes just a single institution name and returns a non-duplicate list of all of the faculty with said institution in their first author affiliation on ADS.
expertiseFinder_NameOrInst(): This function was designed to determine the astronomical expertises of the top first authors from a desired institution. It combines the functionality of expertiseFinder_singleName() and instFinder(). It is best used when researching those affiliated with a single institution. Again, it has customizable filters. This function returns a single spreadsheet with “dirty” and “clean” data labelled in a final column. 
IN THE TWITTER FILE--
expertiseFinderTwitter(): This is the most important component function of the main function within this file, twitterDataFinder(). This function is different than MSI expertise finders in that it is meant to deal with a list of Twitter user names WITHOUT institution data, as opposed to a list of neatly organized researcher names each with an affiliate institution. As a result, it does not eliminate any ADS results based on affiliation data: it only looks at the accuracy of names in resultant queries. There is no customization available for this function.

—THE STEPS—
One may notice that around half of each .py file contains functions with names such as “step4”, step6”, etc. These functions are the building blocks for all of the expertise finder functions.
step12(): Load and truncate the input file based on start and count values.
step3(): Use the ads package’s function ads.SearchQuery to search NASA ADS API for paper information.
step4(): Collects bibcode metadata on each paper found in step3().
step5(): Pulls out all other relevant metadata for each bibcode found in step4().
step6(): This is the step in which filters are applied to all of the papers and their metadata found in step5(). Depending on strictness choices in some functions to decide which version will be used: step6_expertiseFinder(), step6_expertiseFinderNameInst(), step6_expertiseFinder_singleName(), or step6_mostStrict().
step7(): This step merges all of the clean and dirty data into their own spreadsheets, and merges the metadata for every paper by each author into single strings to prepare for analysis.
step8(): Analyzes the clean and dirty data for each author’s papers for top words, bi-grams, and tri-grams.

—THE CLEANERS—
This collection of functions work to “clean up” the outputs from expertise finders. For those expertise finders with two outputs, these functions compare the dirty and clean data to the original input file to find what names successfully found clean data, which names only had dirty data in ADS, and which names turned up no results and are missing from both the dirty and clean spreadsheets. Both return three items: dirtyDf, containing the names and info of authors that only had dirty data; cleanDf, containing the names and info of authors that had clean data; missingNames, containing the names of authors who had no data at all in ADS.
dirtyCleaner_moreStrict(): This function removes names from dirtyDf that are present in cleanDf. It also returns a list of names that are missing from both.
dirtyCleaner_moreStrict(): This function keeps names in cleanDf that also appear in dirtyDf. It also returns a list of names that are missing from both.
(The same logic applies to the "dirty cleaners" in the Twitter .py file.)

-TWITTER EXTRAS-
In this section, I discuss some extra functions within the file "ExpertiseFinder_Twitter.py" that are important to explain.
cleanTwitterData(): this function was created to address just how inconsistent data pulled from Twitter can be. In order for expertiseFinderTwitter() to work, it needs to have a list of "LastName, FirstName" strings to run through ADS. Unfortunately, not a lot of users on Twitter write their names like that. Many people may title their twitter pages with just their first names, or with a collection of emojis, or begin with "Dr..." or "Professor...". The function cleans up the "Name" column within a spreadsheet of Twitter data and eliminates any rows that most likely do not have both first and last names within them. It does this by dropping any rows with only one word in the Twitter name string. It will also remove excess characters such as emoticons or punctuation or suffixes. Then, if the string is two words long, it will reorder them to match the "LastName, FirstName" format.
twitterDataFinder(): This is technically the main function for this .py file and NOT expertiseFinderTwitter(). It combines the functionality of expertiseFinderTwitter() and dirtycleaner_moreStrict_Twitter(), which results in a single return spreadsheet as opposed to two. 

——————————————————————————————————————————————————

I would like to give special thanks to Antonino Cucchiara, my supervisor and guide at NASA, who offered invaluable guidance and input while I developed this code. I would also like to thank the great people at NASA HQ APD for their constant encouragement and support. Thank you to USRA and the internship coordinators for offering such a special opportunity to work at NASA HQ for almost a full year. This experience was life-changing. 

Please do not hesitate to contact me via GitHub with any comments, questions, concerns, or bugs. Happy coding !
